{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import models\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2131 files belonging to 4 classes.\n",
      "Found 2131 files belonging to 4 classes.\n",
      "Using 1705 files for training.\n",
      "Found 2131 files belonging to 4 classes.\n",
      "Using 426 files for validation.\n"
     ]
    }
   ],
   "source": [
    "#directory = \"/Users/abhar/Downloads/face_age/face_age_image\" \n",
    "\n",
    "directory = \"/Users/abhar/Downloads/practice_imageset\" #practice data set\n",
    "\n",
    "dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        class_names=None,\n",
    "        color_mode=\"rgb\",\n",
    "        batch_size=16,\n",
    "        image_size=(200, 200),\n",
    "\n",
    ")\n",
    "\n",
    "train_x_ds = tf.keras.utils.image_dataset_from_directory(directory,validation_split=0.2,subset=\"training\",seed = 42)\n",
    "validation_x_ds = tf.keras.utils.image_dataset_from_directory(directory,validation_split=0.2,subset=\"validation\",seed = 42)\n",
    "\n",
    "\n",
    "\n",
    "#testx_ds = tf.keras.utils.image_dataset_from_directory(directory,validation_split=0.3,subset=\"validation\",seed = 42)\n",
    "#testy_ds = tf.keras.utils.image_dataset_from_directory(directory,validation_split=0.3,subset=\"validation\",seed = 42)\n",
    "#print(dataset)\n",
    "\n",
    "#for i in dataset:\n",
    "    #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 256, 256, 25)      1900      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 128, 128, 25)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 50)        31300     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 50)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 50)        200       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 70)        31570     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 70)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 70)          280       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4480)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               448100    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 524,460\n",
      "Trainable params: 524,220\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n",
      "[INFO] training model...\n",
      "Epoch 1/10\n",
      "47/47 [==============================] - 114s 2s/step - loss: 53083160.0000 - val_loss: 53083168.0000\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 107s 2s/step - loss: 53083156.0000 - val_loss: 53083156.0000\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 116s 2s/step - loss: 53083156.0000 - val_loss: 53083156.0000\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 115s 2s/step - loss: 53083144.0000 - val_loss: 53083156.0000\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 110s 2s/step - loss: 53083156.0000 - val_loss: 53083156.0000\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 118s 3s/step - loss: 53083148.0000 - val_loss: 53083156.0000\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 96s 2s/step - loss: 53083156.0000 - val_loss: 53083160.0000\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 93s 2s/step - loss: 53083156.0000 - val_loss: 53083160.0000\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 95s 2s/step - loss: 53083156.0000 - val_loss: 53083156.0000\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 102s 2s/step - loss: 53083156.0000 - val_loss: 53083156.0000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`y` argument is not supported when using dataset as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-60ed972d8c82>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m#y_pred = model.predict(testx_ds)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1476\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1477\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1478\u001b[1;33m             steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1480\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1381\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cluster_coordinator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1382\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1383\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, steps, **kwargs)\u001b[0m\n\u001b[0;32m    706\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_user_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_validate_args\u001b[1;34m(self, y, sample_weights, steps)\u001b[0m\n\u001b[0;32m    739\u001b[0m     \u001b[1;31m# Arguments that shouldn't be passed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    740\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_none_or_empty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 741\u001b[1;33m       raise ValueError(\"`y` argument is not supported when using \"\n\u001b[0m\u001b[0;32m    742\u001b[0m                        \"dataset as input.\")\n\u001b[0;32m    743\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_none_or_empty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: `y` argument is not supported when using dataset as input."
     ]
    }
   ],
   "source": [
    "#Make model\n",
    "model = Sequential()\n",
    "model.add(layers.InputLayer(input_shape=(256, 256, 3)))\n",
    "\n",
    "# 1st conv block\n",
    "model.add(layers.Conv2D(25, (5, 5), activation='relu', strides=(1, 1), padding='same'))\n",
    "model.add(layers.MaxPool2D(pool_size=(2, 2), padding='same'))\n",
    "# 2nd conv block\n",
    "model.add(layers.Conv2D(50, (5, 5), activation='relu', strides=(2, 2), padding='same'))\n",
    "model.add(layers.MaxPool2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(layers.BatchNormalization())\n",
    "# 3rd conv block\n",
    "model.add(layers.Conv2D(70, (3, 3), activation='relu', strides=(2, 2), padding='same'))\n",
    "model.add(layers.MaxPool2D(pool_size=(2, 2), padding='valid'))\n",
    "model.add(layers.BatchNormalization())\n",
    "# ANN block\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(units=100, activation='relu'))\n",
    "model.add(layers.Dense(units=100, activation='relu'))\n",
    "model.add(layers.Dropout(0.25))\n",
    "# output layer\n",
    "model.add(layers.Dense(units=10, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "opt = Adam(learning_rate=1e-3, decay=1e-3 / 200)\n",
    "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n",
    "# train the model\n",
    "print(\"[INFO] training model...\")\n",
    "im_model = model.fit(x=train_x_ds,epochs=10,validation_data=#should be the y test data here,batch_size=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "))\n",
    "\n",
    "#model.fit(x=train_ds, \n",
    "    #validation_data=(testImagesX, testY),\n",
    "    #epochs=200, batch_size=8)\n",
    "\n",
    "\n",
    "#model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "#history = model.fit(x=train_ds,epochs=10,validation_data=val_ds)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2ab331ed9f1f7150fb7ff71f78d22457345ce6bc704fab419f086a2a4dd6854"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
